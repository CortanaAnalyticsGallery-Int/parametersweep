# Spark configuration and packages specification. The dependencies defined in
# this file will be automatically provisioned for each run that uses Spark.

# Spark configuration values can be set through the configuration dictionary.
# Spark packages can be added through the repositories and packages lists.

# For third-party python libraries, see conda_dependencies.yml.

configuration:
  "spark.app.name": "Parameter Sweep"
repositories:
  - "https://azuremlbuild.blob.core.windows.net/maven"
  - "https://spark-packages.org/packages"
packages:
  - group: "com.microsoft.ml.spark"
    artifact: "mmlspark_2.11"
    version: "0.4"
  - group: "databricks"
    artifact: "spark-sklearn"
    version: "0.2.0"